# Directive: Testing Policy

## Purpose

Define ownership boundaries between human-written spec tests and AI-generated implementation tests so the AI cannot "cheat" its way to passing tests.

## Trigger

- [x] Manual (when writing tests or when a test fails)
- [x] Manual (before the AI generates any new test file)

## Inputs

| Name | Type | Required | Description |
|------|------|----------|-------------|
| Feature or module being tested | string | Yes | What area of the app needs tests |

## Test Ownership Model

There are two categories of tests with different rules. The distinction is enforced by both **file location** and **naming convention**.

### Spec Tests (Human-Owned)

| Property | Value |
|----------|-------|
| Location | `tests/specs/` at project root |
| Naming | `*.spec.ts` |
| Owner | Human |
| AI can edit | **No — never** |

Spec tests define the **contract** — what the system must do. They are the source of truth for correctness.

- Written by a human (or generated by an AI in a separate context with zero implementation knowledge, then reviewed and adopted by a human)
- Test public interfaces and observable behavior, not internal implementation details
- Mock only external dependencies (databases, third-party APIs). Never mock the system under test.
- Use realistic inputs, not values copied from the implementation
- If a spec test fails, the **implementation is wrong**, not the test

**Example:**

```typescript
// tests/specs/billing.spec.ts
// HUMAN-OWNED — AI must not edit this file

import { calculateInvoiceTotal } from "@/lib/api/billing";

describe("calculateInvoiceTotal", () => {
  it("sums line items and applies tax", () => {
    const result = calculateInvoiceTotal({
      items: [
        { description: "Widget", quantity: 2, unitPrice: 10.0 },
        { description: "Gadget", quantity: 1, unitPrice: 25.0 },
      ],
      taxRate: 0.08,
    });

    expect(result.subtotal).toBe(45.0);
    expect(result.tax).toBe(3.6);
    expect(result.total).toBe(48.6);
  });

  it("returns zero for an empty invoice", () => {
    const result = calculateInvoiceTotal({ items: [], taxRate: 0.08 });

    expect(result.subtotal).toBe(0);
    expect(result.tax).toBe(0);
    expect(result.total).toBe(0);
  });
});
```

### Implementation Tests (AI-Allowed)

| Property | Value |
|----------|-------|
| Location | Colocated with source (e.g., `lib/utils/format.test.ts`) |
| Naming | `*.test.ts` |
| Owner | AI or human |
| AI can edit | **Yes** |

Implementation tests cover internal details, edge cases, and helper functions. They complement spec tests — they don't replace them.

- AI can freely create and modify these
- Must follow conventions in `directives/coding-standards.md`
- Should not duplicate what spec tests already cover
- Can use mocks and stubs as needed

## Directory Structure

```
├── tests/
│   └── specs/              # Human-owned spec tests (AI cannot edit)
│       ├── billing.spec.ts
│       ├── auth.spec.ts
│       └── users.spec.ts
├── lib/
│   ├── api/
│   │   ├── billing.ts
│   │   └── billing.test.ts  # AI-allowed implementation tests
│   └── utils/
│       ├── format.ts
│       └── format.test.ts   # AI-allowed implementation tests
└── e2e/                     # E2E tests (per coding-standards.md)
```

## Anti-Cheat Rules

The AI must **never** do any of the following to make a failing test pass:

| Violation | Why It's Wrong |
|-----------|----------------|
| Edit a `.spec.ts` file | Spec tests are the contract. Changing the contract to match a broken implementation defeats the purpose. |
| Add mocks that bypass the behavior being tested | Mocking the system under test hides bugs instead of fixing them. |
| Hard-code return values to match assertions | This creates code that passes tests but doesn't actually work. |
| Skip or disable a failing spec test | `it.skip()` or `xdescribe()` on a spec test is never acceptable. |
| Move a spec test out of `tests/specs/` | Relocating it to bypass the ownership rule is the same as editing it. |
| Copy a spec test to a `.test.ts` file and modify the copy | This creates a shadow test that undermines the original spec. |

If the AI encounters a failing spec test, the **only correct response** is to fix the implementation.

## Steps

### When the AI needs to write tests

1. Check if a spec test already exists in `tests/specs/` for the feature
2. If yes, run the spec tests first to understand the expected behavior
3. Write implementation tests (`.test.ts`) that cover edge cases and internals not covered by the spec
4. Never duplicate assertions that spec tests already cover

### When a spec test fails

1. **Stop.** Do not edit the spec test.
2. Read the spec test to understand the expected behavior
3. Read the implementation to find the bug
4. Fix the implementation
5. Run the spec test again to verify
6. If you believe the spec test itself is wrong (e.g., outdated requirement), tell the user: "Spec test `[name]` is failing. I believe the spec may be outdated because [reason]. Can you review and update it?"

### When writing spec tests (human workflow)

Guidance for when a human writes spec tests (or asks the AI to draft them in a separate context):

1. Write specs based on **requirements**, not implementation
2. Don't look at the current implementation while writing — test the interface contract
3. Use realistic, meaningful test data (not values copied from source code)
4. Cover the happy path, edge cases, and error conditions
5. Place in `tests/specs/` with the `.spec.ts` extension
6. Add the comment `// HUMAN-OWNED — AI must not edit this file` at the top

## Outputs

- Spec tests in `tests/specs/*.spec.ts` (human-created)
- Implementation tests colocated as `*.test.ts` (AI-created)
- Clear separation between "what must work" (specs) and "how it works internally" (implementation tests)

## Tools Used

- Testing framework defined in `directives/coding-standards.md` (Vitest/Jest, Playwright)

## Error Handling

| Error | Recovery |
|-------|----------|
| AI edits a `.spec.ts` file | Self-anneal: revert the edit, log the error, notify the user |
| Spec test fails and AI can't figure out why | Ask the user for help rather than working around the test |
| No spec tests exist for a feature | Notify the user: "No spec tests exist for [feature]. Would you like to write them before I implement?" |
| AI-generated implementation test is flaky | Investigate root cause. Flaky tests are bugs — don't retry-and-hope. |

## Learned Constraints

<!--
  Updated by the agent during self-annealing.
  Each entry must include a date and be appended, never overwritten.
  Format: - YYYY-MM-DD: What was learned
-->
